{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnxEw7MVK2mYRhkOY71oS9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NYGCEDRIC/LLM_ZOOMCAMP/blob/main/LLm_Zoomcamp_Model_2_Homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1. Running Ollama with Docker. What's the version? (1 point)\n",
        "\n",
        "ollama version is 0.2.1\n"
      ],
      "metadata": {
        "id": "ZJy-p-ZdMjqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2. Downloading an LLM. Manifest file (1 point)\n",
        "-## Q2. Gemma:2b Model Metadata\n",
        "```json\n",
        "{\n",
        "  \"schemaVersion\": 2,\n",
        "  \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\",\n",
        "  \"config\": {\n",
        "    \"mediaType\": \"application/vnd.docker.container.image.v1+json\",\n",
        "    \"digest\": \"sha256:887433b89a901c156f7e6944442f3c9e57f3c55d6ed52042cbb7303aea994290\",\n",
        "    \"size\": 483\n",
        "  },\n",
        "  \"layers\": [\n",
        "    {\n",
        "      \"mediaType\": \"application/vnd.ollama.image.model\",\n",
        "      \"digest\": \"sha256:c1864a5eb19305c40519da12cc543519e48a0697ecd30e15d5ac228644957d12\",\n",
        "      \"size\": 1678447520\n",
        "    },\n",
        "    {\n",
        "      \"mediaType\": \"application/vnd.ollama.image.license\",\n",
        "      \"digest\": \"sha256:097a36493f718248845233af1d3fefe7a303f864fae13bc31a3a9704229378ca\",\n",
        "      \"size\": 8433\n",
        "    },\n",
        "    {\n",
        "      \"mediaType\": \"application/vnd.ollama.image.template\",\n",
        "      \"digest\": \"sha256:109037bec39c0becc8221222ae23557559bc594290945a2c4221ab4f303b8871\",\n",
        "      \"size\": 136\n",
        "    },\n",
        "    {\n",
        "      \"mediaType\": \"application/vnd.ollama.image.params\",\n",
        "      \"digest\": \"sha256:22...\"\n",
        "    }\n",
        "  ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "T-5_viAaMrI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3. Running the LLM. Output from 10 * 10\n",
        "\n",
        "-- Answer: \"10 * 10\"\n"
      ],
      "metadata": {
        "id": "IR0Ytz75NbED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4. Downloading the weights. Size of the folder\n",
        "\n",
        "-(base) cedricnyagatare@Cedrics-MacBook-Pro ollama_project % docker run -it --rm -p 11434:11434 ollama-gemma2b\n",
        "\n",
        "Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\n",
        "Your new public key is:\n",
        "\n",
        "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIB92nvVp1hiAEHGizS0xkPE+irUnqeaBJfo0FWz4aJvJ\n",
        "\n",
        "2024/07/11 00:50:46 routes.go:1033: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS:/root/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\n",
        "time=2024-07-11T00:50:46.647Z level=INFO source=images.go:751 msg=\"total blobs: 0\"\n",
        "time=2024-07-11T00:50:46.647Z level=INFO source=images.go:758 msg=\"total unused blobs removed: 0\"\n",
        "time=2024-07-11T00:50:46.648Z level=INFO source=routes.go:1080 msg=\"Listening on [::]:11434 (version 0.2.1)\"\n",
        "time=2024-07-11T00:50:46.649Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama4012613131/runners\n",
        "time=2024-07-11T00:50:52.349Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [rocm_v60101 cpu cpu_avx cpu_avx2 cuda_v11]\"\n",
        "time=2024-07-11T00:50:52.349Z level=INFO source=gpu.go:205 msg=\"looking for compatible GPUs\"\n",
        "time=2024-07-11T00:50:52.352Z level=INFO source=gpu.go:324 msg=\"no compatible GPUs were discovered\"\n",
        "time=2024-07-11T00:50:52.352Z level=INFO source=types.go:103 msg=\"inference compute\" id=0 library=cpu compute=\"\" driver=0.0 name=\"\" total=\"7.7 GiB\" available=\"7.1 GiB\"\n",
        "\n",
        "-Size of the folder is 7.1 GB"
      ],
      "metadata": {
        "id": "U2TPuYsKO3iM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "brQn-P7YSryg"
      }
    }
  ]
}